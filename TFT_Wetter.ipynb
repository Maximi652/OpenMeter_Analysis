{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_lightning.tuner.lr_finder import _lr_find\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade zu den Daten\n",
    "data_path = r\"C:\\Users\\maximilian.vanliende\\Downloads\\train_data.csv\"\n",
    "test_path = r\"C:\\Users\\maximilian.vanliende\\Downloads\\test_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c7a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maximilian.vanliende\\AppData\\Local\\Temp\\ipykernel_20748\\2794120213.py:2: DtypeWarning: Columns (19,20,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(data_path, parse_dates=[\"Zeitstempel\"])\n",
      "C:\\Users\\maximilian.vanliende\\AppData\\Local\\Temp\\ipykernel_20748\\2794120213.py:3: DtypeWarning: Columns (19,20,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df  = pd.read_csv(test_path,  parse_dates=[\"Zeitstempel\"])\n"
     ]
    }
   ],
   "source": [
    "# Daten einlesen\n",
    "train_df = pd.read_csv(data_path, parse_dates=[\"Zeitstempel\"])\n",
    "test_df  = pd.read_csv(test_path,  parse_dates=[\"Zeitstempel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten kombinieren und Split-Kennzeichnung\n",
    "df = pd.concat(\n",
    "    [train_df.assign(split=\"train\"), test_df.assign(split=\"test\")],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd5fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maximilian.vanliende\\AppData\\Local\\Temp\\ipykernel_20748\\1495730425.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Ferientyp'].fillna('None', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Zeitmerkmale und Kategorien (nur Wetter)\n",
    "df[\"meter_id\"] = df[\"location_id\"]\n",
    "\n",
    "# Zeitmerkmale extrahieren\n",
    "for part in (\"month\", \"weekday\", \"hour\"):\n",
    "    df[part] = getattr(df[\"Zeitstempel\"].dt, part).astype(str)\n",
    "\n",
    "# Feiertags-Information\n",
    "df[\"is_holiday\"] = df[\"Ferientyp\"].notna().astype(str)\n",
    "df[\"Ferientyp\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# Nur Wetterkategorien als kategorisch kennzeichnen\n",
    "for col in [\"is_holiday\", \"month\", \"weekday\", \"hour\", \"Ferientyp\", \"Feiertag\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "\n",
    "# Statische und zeitabhängige Merkmale (ohne Geodaten)\n",
    "static_categoricals             = []  # keine Geo-Kategorien\n",
    "static_reals                    = []  # keine Geo-Numerics\n",
    "time_varying_known_categoricals = [\"is_holiday\", \"month\", \"weekday\", \"hour\"]\n",
    "time_varying_known_reals        = [\"time_idx\", \"cloud_cover_total\", \"humidity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statische numerische Merkmale (nur Wetter-Füllung)\n",
    "real_fill_cols = [\"cloud_cover_total\", \"humidity\"]\n",
    "for col in real_fill_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].ffill().bfill().fillna(df[col].median())\n",
    "\n",
    "# Unnötige Spalten entfernen und Duplikate löschen\n",
    "df.drop(columns=[\"Unnamed: 0\", \"Kreis code\", \"Kreis name\"], errors=\"ignore\", inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation auf Zeitindex und Standort\n",
    "cat_cols_group = [\"location_id\", \"split\", \"meter_id\"] + \\\n",
    "    [c for c in [\"is_holiday\", \"month\", \"weekday\", \"hour\", \"Ferientyp\", \"Feiertag\"] if c in df.columns]\n",
    "num_cols_group = df.select_dtypes(include=\"number\").columns.difference([\"location_id\", \"Zeitstempel\"])\n",
    "\n",
    "agg_dict = {col: \"first\" for col in cat_cols_group if col in df.columns}\n",
    "agg_dict.update({col: \"mean\" for col in num_cols_group if col in df.columns})\n",
    "\n",
    "df = df.groupby([\"location_id\", \"Zeitstempel\"], as_index=False).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37583f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitindex erstellen\n",
    "unique_times = pd.Series(df[\"Zeitstempel\"].unique()).sort_values().reset_index(drop=True)\n",
    "time_map = {time: idx for idx, time in enumerate(unique_times)}\n",
    "df[\"time_idx\"] = df[\"Zeitstempel\"].map(time_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab204dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split zurücksetzen\n",
    "train_df = df[df.split == \"train\"].drop(columns=\"split\")\n",
    "test_df  = df[df.split == \"test\"].drop(columns=\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesDataSet erstellen\n",
    "max_prediction_length = 96   # Vorhersagehorizont (~1 Tag)\n",
    "max_encoder_length    = 288  # Encoder-Länge (~3 Tage)\n",
    "training_cutoff       = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[df.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Messwert\",\n",
    "    group_ids=[\"meter_id\"],\n",
    "    static_categoricals=static_categoricals,\n",
    "    static_reals=static_reals,\n",
    "    time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=[\"Messwert\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    min_prediction_length=1,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"meter_id\"], transformation=\"softplus\"\n",
    "    ),\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    allow_missing_timesteps=True,\n",
    "    add_relative_time_idx=True,\n",
    "    add_encoder_length=True,\n",
    "    add_target_scales=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, df, predict=True, stop_randomization=True\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=6, persistent_workers=True\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fe114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 0.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# Basis-Baseline berechnen (letzter Wert als Vorhersage)\n",
    "actuals = torch.cat([y for x, (y, _) in iter(val_dataloader)])\n",
    "baseline_preds = Baseline().predict(val_dataloader)\n",
    "baseline_mae = (actuals - baseline_preds).abs().mean().item()\n",
    "print(f\"Baseline MAE: {baseline_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 748    | train\n",
      "3  | prescalers                         | ModuleDict                      | 576    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 28.1 K | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 11.0 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.9 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 16.9 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 16.9 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 231    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.474     Total estimated model params size (MB)\n",
      "533       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Parameter im Modell: 118.6k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b54550eac34e06bc0aac576af4086d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\maximilian.vanliende\\AppData\\Local\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8910f1f6f7fe47ef9e72a3e44c3d946b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12aca02b7a24fa89dc711f181132fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854183a89a82438a84caf05d96cb2a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fa914d85eb4dffa84ef8a14f6f6dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Modelltraining konfigurieren\n",
    "pl.seed_everything(42)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=10, mode=\"min\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "logger     = CSVLogger(\"logs_csv\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    precision=\"64\",\n",
    "    devices=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[lr_monitor, early_stop],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-4,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    lstm_layers=2,\n",
    "    dropout=0.2,\n",
    "    hidden_continuous_size=16,\n",
    "    optimizer=\"adam\",\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=1,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Anzahl Parameter im Modell: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f541ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Finder\n",
    "lr_find_result = _lr_find(\n",
    "    model=tft,\n",
    "    trainer=trainer,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    "    num_training=100\n",
    ")\n",
    "print(f\"Empfohlene Lernrate: {lr_find_result.suggestion():.2e}\")\n",
    "fig = lr_find_result.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Metriken berechnen\n",
    "tft = tft.float()\n",
    "tft.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = tft.predict(val_dataloader)\n",
    "    preds = torch.from_numpy(preds).float() if not isinstance(preds, torch.Tensor) else preds\n",
    "\n",
    "actuals = torch.cat([y for x, (_, _) in iter(val_dataloader)]).float()\n",
    "\n",
    "preds_flat   = preds.numpy().flatten()\n",
    "actuals_flat = actuals.numpy().flatten()\n",
    "\n",
    "mse               = mean_squared_error(actuals_flat, preds_flat)\n",
    "rmse              = mse**0.5\n",
    "mae               = mean_absolute_error(actuals_flat, preds_flat)\n",
    "smape_val         = SMAPE()(preds, actuals).item()\n",
    "quantile_loss_val = QuantileLoss()(preds, actuals).item()\n",
    "\n",
    "print(f\"MSE:           {mse:.4f}\")\n",
    "print(f\"RMSE:          {rmse:.4f}\")\n",
    "print(f\"MAE:           {mae:.4f}\")\n",
    "print(f\"SMAPE:         {smape_val:.2f}%\")\n",
    "print(f\"Quantile Loss: {quantile_loss_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots: Beispielserie Vorhersage vs. Ist\n",
    "tft.eval()\n",
    "with torch.no_grad():\n",
    "    preds_all  = tft.predict(val_dataloader)\n",
    "actuals_all = torch.cat([y for x, (y, _) in iter(val_dataloader)])\n",
    "\n",
    "series_idx  = 0\n",
    "true_series = actuals_all[series_idx].numpy()\n",
    "pred_series = preds_all[series_idx].numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_series, label=\"Tatsächlich\")\n",
    "plt.plot(pred_series, label=\"Vorhersage (Median)\")\n",
    "plt.legend()\n",
    "plt.title(\"Ist vs. Vorhersage für eine Serie\")\n",
    "plt.xlabel(\"Zeitschritt\")\n",
    "plt.ylabel(\"Messwert\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuen über die Vorhersageperiode\n",
    "residuals = true_series - pred_series\n",
    "plt.figure()\n",
    "plt.plot(residuals)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(\"Residuen über die Vorhersageperiode\")\n",
    "plt.xlabel(\"Zeitschritt\")\n",
    "plt.ylabel(\"Residuum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlerverteilung (Histogramm)\n",
    "all_errors = (actuals_all.numpy().flatten() - preds_all.numpy().flatten())\n",
    "plt.figure()\n",
    "plt.hist(all_errors, bins=50)\n",
    "plt.title(\"Verteilung der Vorhersagefehler\")\n",
    "plt.xlabel(\"Fehler (Ist – Vorh.)\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile-Bänder (10%, 50%, 90%)\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "tft.eval()\n",
    "with torch.no_grad():\n",
    "    preds_q = tft.predict(\n",
    "        val_dataloader,\n",
    "        mode=\"quantiles\",\n",
    "        mode_kwargs={\"quantiles\": quantiles},\n",
    "    )\n",
    "\n",
    "actuals_all = torch.cat([y for x, (y, _) in iter(val_dataloader)])\n",
    "q_series    = preds_q[series_idx]\n",
    "lower       = q_series[:, 0].numpy()\n",
    "median      = q_series[:, 1].numpy()\n",
    "upper       = q_series[:, 2].numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(median, label=\"Median\")\n",
    "plt.fill_between(range(len(median)), lower, upper, alpha=0.2)\n",
    "plt.plot(true_series, label=\"Tatsächlich\")\n",
    "plt.legend()\n",
    "plt.title(\"Quantile‑Bänder (10%–90%) vs. Median\")\n",
    "plt.xlabel(\"Zeitschritt\")\n",
    "plt.ylabel(\"Messwert\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitachse mit Datumsformat\n",
    "start_date = pd.Timestamp(\"2018-01-01\")  # Anpassen auf ersten Messmonat\n",
    "dates      = pd.date_range(start_date, periods=len(true_series), freq=\"M\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(dates, true_series, label=\"Tatsächlich\")\n",
    "ax.plot(dates, pred_series, label=\"Vorhersage (Median)\")\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "ax.set_title(\"Ist vs. Vorhersage für eine Serie\")\n",
    "ax.set_xlabel(\"Zeit\")\n",
    "ax.set_ylabel(\"Messwert\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
